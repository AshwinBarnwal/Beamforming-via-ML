{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9da2c04",
   "metadata": {},
   "source": [
    "Using MVDR as the beamformer, utilizing some/different neural networks pretrained or otherwise, to predict masks for the frequency and time bins to finetune the calculation of spatial covariances for each time chunk.\n",
    "\n",
    "The beamformer is made adaptive by including a forgetting factor that updates the spatial covariance as a function of the previous time chunks and the one calculated solely from the current time chunk. Beamformer weights are calculated individually for every such chunk then applied solely to the STFT values of that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30879768",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.signal import stft, istft, windows\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "audio_path = '/kaggle/input/audios/audio_dataset/audio_dataset/samsung_non_overlapping/02-25.12-20-54-168__WL_BH_d1m_Left_TC5.hdf/02-25.12-20-54-168__WL_BH_d1m_Left_TC5.wav'\n",
    "audio, fs = sf.read(audio_path)  # Expect shape (n_samples, 4)\n",
    "assert fs == 16000, f\"Expected sampling rate 16000, but got {fs}\"\n",
    "assert audio.ndim == 2 and audio.shape[1] == 4, \"Audio must be 4-channel\"\n",
    "audio = np.asarray(audio, dtype=np.float32)\n",
    "print(f\"Loaded audio with shape {audio.shape}, fs = {fs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083398a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mic_positions = np.array([\n",
    "    [-0.05,  0.00,  0.00],  # Mic 1 (right)\n",
    "    [ 0.05,  0.00,  0.00],  # Mic 2 (left)\n",
    "    [-0.08,  0.045, 0.04],  # Mic 3 (upper-right-back)\n",
    "    [ 0.08,  0.045, 0.04],  # Mic 4 (upper-left-back)\n",
    "], dtype=np.float32)\n",
    "\n",
    "source_pos = np.array([0.0, -0.06, 0.0], dtype=np.float32)\n",
    "\n",
    "# Far-field noise DoA: example azimuth in horizontal (x-z) plane, 0° = front (-z)\n",
    "# User can update doa_noise_deg per segment/frame if dynamic\n",
    "doa_noise_deg = 90.0  \n",
    "az = np.deg2rad(doa_noise_deg)\n",
    "doa_noise_vec = np.array([np.sin(az), 0.0, -np.cos(az)], dtype=np.float32)\n",
    "doa_noise_vec /= np.linalg.norm(doa_noise_vec)\n",
    "\n",
    "print(\"Mic positions:\\n\", mic_positions)\n",
    "print(\"Source position:\", source_pos)\n",
    "print(f\"Noise DoA vector (az={doa_noise_deg}°):\", doa_noise_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb84cb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_fft = 512\n",
    "hop_length = n_fft // 2\n",
    "window = windows.hann(n_fft, sym=False)\n",
    "\n",
    "# Compute STFT for each channel\n",
    "stfts = []\n",
    "for ch in range(4):\n",
    "    f_bins, t_frames, Zxx = stft(\n",
    "        audio[:, ch], fs=fs, window=window,\n",
    "        nperseg=n_fft, noverlap=n_fft-hop_length,\n",
    "        boundary=None, padded=False\n",
    "    )\n",
    "    stfts.append(Zxx)  # shape (F, T)\n",
    "# Stack to shape (F, T, M)\n",
    "stfts = np.stack(stfts, axis=2)  # (F_bins, T_frames, 4)\n",
    "F_bins, T_frames, M = stfts.shape\n",
    "assert M == 4\n",
    "freqs = f_bins  # array of length F_bins\n",
    "print(f\"STFT computed: freq bins={F_bins}, time frames={T_frames}, channels={M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb33e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "c = 343.0  # speed of sound\n",
    "\n",
    "def steering_vector_nearfield(mic_positions, source_pos, freqs, speed_of_sound=343.0, include_amplitude=False):\n",
    "    \"\"\"\n",
    "    Compute near-field steering vectors for each frequency.\n",
    "    Returns array shape (F_bins, M).\n",
    "    \"\"\"\n",
    "    diffs = mic_positions - source_pos[None, :]  # (M,3)\n",
    "    dists = np.linalg.norm(diffs, axis=1)       # (M,)\n",
    "    dists = np.maximum(dists, 1e-6)\n",
    "    F = len(freqs)\n",
    "    M = mic_positions.shape[0]\n",
    "    a = np.zeros((F, M), dtype=np.complex64)\n",
    "    for idx, f in enumerate(freqs):\n",
    "        phase = np.exp(-1j * 2 * np.pi * f * dists / speed_of_sound)\n",
    "        if include_amplitude:\n",
    "            a[idx, :] = phase / dists\n",
    "        else:\n",
    "            a[idx, :] = phase\n",
    "    return a\n",
    "\n",
    "def steering_vector_farfield(mic_positions, doa_vec, freqs, speed_of_sound=343.0):\n",
    "    \"\"\"\n",
    "    Compute far-field steering vectors for each frequency.\n",
    "    Returns array shape (F_bins, M).\n",
    "    \"\"\"\n",
    "    proj = mic_positions.dot(doa_vec)  # (M,)\n",
    "    F = len(freqs)\n",
    "    M = mic_positions.shape[0]\n",
    "    a = np.zeros((F, M), dtype=np.complex64)\n",
    "    for idx, f in enumerate(freqs):\n",
    "        a[idx, :] = np.exp(-1j * 2 * np.pi * f * proj / speed_of_sound)\n",
    "    return a\n",
    "\n",
    "# Precompute steering vectors\n",
    "a_s = steering_vector_nearfield(mic_positions, source_pos, freqs, speed_of_sound=c, include_amplitude=False)  # (F_bins, M)\n",
    "a_n = steering_vector_farfield(mic_positions, doa_noise_vec, freqs, speed_of_sound=c)  # (F_bins, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc48e07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# placeholder LSTM network, not currently tested, ideally require pretrained checkpoints\n",
    "\n",
    "class MaskNet(nn.Module):\n",
    "    def __init__(self, n_freq_bins, n_ipd_pairs):\n",
    "        super(MaskNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1 + 2 * n_ipd_pairs, out_channels=16, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.blstm = nn.LSTM(input_size=16 * n_freq_bins, hidden_size=128, num_layers=1,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(128 * 2, n_freq_bins)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, mag, ipd):\n",
    "        # mag: (batch, 1, T, F), ipd: (batch, 2*n_ipd_pairs, T, F)\n",
    "        x = torch.cat([mag, ipd], dim=1)  # (batch, channels, T, F)\n",
    "        x = self.conv(x)  # (batch, 16, T, F)\n",
    "        b, c, T, F = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # (batch, T, 16, F)\n",
    "        x = x.view(b, T, c * F)  # (batch, T, 16*F)\n",
    "        y, _ = self.blstm(x)  # (batch, T, 2*hidden)\n",
    "        mask = self.fc(y)  # (batch, T, F)\n",
    "        mask = self.sigmoid(mask)\n",
    "        return mask  # (batch, T, F)\n",
    "\n",
    "# Instantiate MaskNet\n",
    "n_freq_bins = F_bins\n",
    "n_ipd_pairs = M - 1  # pairs relative to ref mic 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mask_net = MaskNet(n_freq_bins=n_freq_bins, n_ipd_pairs=n_ipd_pairs).to(device)\n",
    "# TODO: load pretrained weights ex.:\n",
    "# mask_net.load_state_dict(torch.load('masknet_checkpoint.pth'))\n",
    "mask_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de6bc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stft_tensor = torch.from_numpy(np.transpose(stfts, (2, 1, 0))).unsqueeze(0)  # (1, M, T, F)\n",
    "\n",
    "# Reference magnitude (mic 0)\n",
    "mag = torch.abs(stft_tensor[:, 0:1, :, :])  # (1,1,T,F)\n",
    "\n",
    "# IPD features between mic i and mic 0\n",
    "eps = 1e-9\n",
    "ipd_list = []\n",
    "for i in range(1, M):\n",
    "    Xi = stft_tensor[:, i, :, :].type(torch.complex64)  # (1, T, F)\n",
    "    X0 = stft_tensor[:, 0, :, :].type(torch.complex64)\n",
    "    cs = Xi * torch.conj(X0)  # (1, T, F)\n",
    "    cs_norm = cs / (torch.abs(cs) + eps)\n",
    "    ipd_list.append(torch.real(cs_norm).unsqueeze(1))  # (1,1,T,F)\n",
    "    ipd_list.append(torch.imag(cs_norm).unsqueeze(1))\n",
    "ipd = torch.cat(ipd_list, dim=1)  # (1, 2*(M-1), T, F)\n",
    "\n",
    "# Predict mask (batch size 1)\n",
    "with torch.no_grad():\n",
    "    mag_dev = mag.to(device)\n",
    "    ipd_dev = ipd.to(device)\n",
    "    mask_pred = mask_net(mag_dev, ipd_dev)  # (1, T, F)\n",
    "mask_pred = mask_pred.squeeze(0).cpu().numpy()  # (T_frames, F_bins)\n",
    "print(\"Predicted noise mask shape:\", mask_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340eb8e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.95      # forgetting factor, adjust for adaptivity vs stability\n",
    "diag_loading = 1e-6\n",
    "eps = 1e-9\n",
    "\n",
    "# Initialize recursive noise covariance estimates R_n_est[f] as identity\n",
    "R_n_est = np.array([np.eye(M, dtype=np.complex64) for _ in range(F_bins)])  # (F_bins, M, M)\n",
    "\n",
    "# Allocate output STFT array\n",
    "Y = np.zeros((F_bins, T_frames), dtype=np.complex64)\n",
    "\n",
    "for t in range(T_frames):\n",
    "    X_t = stfts[:, t, :]           # (F_bins, M)\n",
    "    mask_t = mask_pred[t, :]       # (F_bins,)\n",
    "    # Update covariance per frequency\n",
    "    for f in range(F_bins):\n",
    "        x = X_t[f, :].reshape(M, 1)  # (M,1)\n",
    "        R_inst = mask_t[f] * (x @ np.conj(x.T))  # (M,M)\n",
    "        # Recursive update\n",
    "        R_n_est[f] = alpha * R_n_est[f] + (1 - alpha) * R_inst\n",
    "        # Diagonal loading\n",
    "        R_n_est[f] += diag_loading * np.eye(M, dtype=np.complex64)\n",
    "    # Compute MVDR weights per frequency\n",
    "    W_t = np.zeros((F_bins, M), dtype=np.complex64)\n",
    "    for f in range(F_bins):\n",
    "        Rf = R_n_est[f]            # (M,M)\n",
    "        a_s_k = a_s[f, :]          # (M,)\n",
    "        # Invert Rf\n",
    "        try:\n",
    "            Rf_inv = np.linalg.inv(Rf)\n",
    "        except np.linalg.LinAlgError:\n",
    "            Rf_inv = np.linalg.pinv(Rf)\n",
    "        denom = np.vdot(a_s_k, Rf_inv @ a_s_k)\n",
    "        if np.abs(denom) < eps:\n",
    "            w = np.zeros(M, dtype=np.complex64)\n",
    "        else:\n",
    "            w = (Rf_inv @ a_s_k) / denom  # MVDR weight\n",
    "        W_t[f, :] = w\n",
    "    # Apply beamforming for frame t\n",
    "    # Y[f,t] = w(f,t).H @ X(f,t)\n",
    "    Y[:, t] = np.sum(np.conj(W_t) * X_t, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf9f82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "_, output = istft(Y, fs=fs, window=window,\n",
    "                 nperseg=n_fft, noverlap=n_fft-hop_length,\n",
    "                 input_onesided=True, boundary=None)\n",
    "output = output[:audio.shape[0]]\n",
    "# Normalize to avoid clipping\n",
    "max_val = np.max(np.abs(output)) + 1e-9\n",
    "if max_val > 1.0:\n",
    "    output_norm = output / max_val\n",
    "else:\n",
    "    output_norm = output\n",
    "\n",
    "output_path = 'adaptive_mvdr_output.wav'\n",
    "sf.write(output_path, output_norm, fs)\n",
    "print(f\"Adaptive MVDR output written to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
